{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "cell_execution_strategy": "setup",
      "provenance": [],
      "name": "api_to_table"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "import concurrent.futures\n",
        "import google.auth.transport.requests\n",
        "from google.cloud import bigquery\n",
        "from google.auth import compute_engine\n",
        "from google.api_core.exceptions import NotFound\n",
        "from google.cloud import resourcemanager_v3"
      ],
      "metadata": {
        "id": "hwtOHP7WstJn",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1741110347090,
          "user_tz": -330,
          "elapsed": 681,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getCredentials():\n",
        "  credentials, project = google.auth.default()\n",
        "  auth_request = google.auth.transport.requests.Request()\n",
        "  credentials.refresh(auth_request)\n",
        "  return credentials"
      ],
      "metadata": {
        "id": "Iiw2Bx0N34tI",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1741110353581,
          "user_tz": -330,
          "elapsed": 706,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetchDataFromPaginationApi(url, credentials):\n",
        "  all_data = []\n",
        "  next_page_token = None\n",
        "  params = {}\n",
        "  while True:\n",
        "    if next_page_token:\n",
        "      params['pageToken'] = next_page_token\n",
        "\n",
        "    response = requests.get(url, params=params, headers={\n",
        "        'Authorization': f'Bearer {credentials.token}'\n",
        "    })\n",
        "\n",
        "    # Check if the response is successful\n",
        "    if response.status_code != 200:\n",
        "      print(f\"Error to fetch data: {response.status_code}, {response.text}\")\n",
        "      return None\n",
        "    data = response.json()\n",
        "    all_data.extend(data.get('transferConfigs', []))\n",
        "    next_page_token = data.get('nextPageToken')\n",
        "\n",
        "    if not next_page_token:\n",
        "      break\n",
        "    print(f\"fetching with token {next_page_token}\")\n",
        "  return all_data"
      ],
      "metadata": {
        "id": "_GGcIJ-WWFmV",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1741110355717,
          "user_tz": -330,
          "elapsed": 0,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetching the data from Api and return the combined result\n",
        "def fetchDataFromApi(projectId):\n",
        "  projectDetailsUrl = f'https://cloudresourcemanager.googleapis.com/v1/projects/{projectId}'\n",
        "  ancestorDetailsUrl = f'https://cloudresourcemanager.googleapis.com/v1/projects/{projectId}:getAncestry'\n",
        "  jobDetailsUrls = f'https://bigquerydatatransfer.googleapis.com/v1/projects/{projectId}/transferConfigs'\n",
        "  credentials = getCredentials()\n",
        "  print(f\"fetching data for project {projectId}.\")\n",
        "\n",
        "  projectDetails = requests.get(projectDetailsUrl, headers={\n",
        "      'Authorization': f'Bearer {credentials.token}'\n",
        "  })\n",
        "  ancestorDetails = requests.post(ancestorDetailsUrl, headers={\n",
        "      'Authorization': f'Bearer {credentials.token}'\n",
        "  })\n",
        "  scheduledJobDetails = fetchDataFromPaginationApi(jobDetailsUrls, credentials)\n",
        "\n",
        "  data = {};\n",
        "  if projectDetails.status_code == 200:\n",
        "    data[\"projectDetails\"] = projectDetails.json()\n",
        "  else:\n",
        "    print(f\"Failed to fetch the project details. error_code: {projectDetails.status_code}, message: {projectDetails.text}\")\n",
        "\n",
        "  if ancestorDetails.status_code == 200:\n",
        "    data[\"ancestorDetails\"] = ancestorDetails.json()\n",
        "  else:\n",
        "    print(f\"Failed to fetch the project ancestor details. error_code: {ancestorDetails.status_code}, message: {ancestorDetails.text}\")\n",
        "\n",
        "  if scheduledJobDetails is not None:\n",
        "    data[\"scheduledJobDetails\"] = scheduledJobDetails\n",
        "  else:\n",
        "    print(f\"Failed to fetch the scheduled job details. error_code: {scheduledJobDetails.status_code}, message: {scheduledJobDetails.text}\")\n",
        "  print(f\"Fetch complete for project: {projectId}\")\n",
        "\n",
        "  return data"
      ],
      "metadata": {
        "id": "bCHDxJXgujJp",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1741110359195,
          "user_tz": -330,
          "elapsed": 718,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transformProjectData(projectData):\n",
        "  transformedData = {\n",
        "        \"project_number\": projectData.get(\"projectNumber\", None),\n",
        "        \"project_id\": projectData.get(\"projectId\", None),\n",
        "        \"lifecycle_state\": projectData.get(\"lifecycleState\", None),\n",
        "        \"name\": projectData.get(\"name\", None),\n",
        "        \"create_time\": projectData.get(\"createTime\", None),\n",
        "        \"labels\": json.dumps(projectData.get(\"labels\", None)) if projectData.get(\"labels\") else None,\n",
        "        \"parent\": projectData.get(\"parent\", None),\n",
        "        \"tags\": json.dumps(projectData.get(\"tags\", None)) if projectData.get(\"tags\") else None\n",
        "  }\n",
        "  return transformedData\n"
      ],
      "metadata": {
        "id": "ayB4rozl1kt1",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1741110361860,
          "user_tz": -330,
          "elapsed": 767,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transformAncestorsData(ancestorsData):\n",
        "  transformed_data = []\n",
        "  for data in ancestorsData['ancestor']:\n",
        "    ancestor = {\n",
        "        \"resource_id\": data.get(\"resourceId\", None)\n",
        "    }\n",
        "    transformed_data.append(ancestor)\n",
        "  return {\n",
        "      'ancestor': transformed_data\n",
        "  }"
      ],
      "metadata": {
        "id": "Un8hWUbJ3c4C",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1741110363945,
          "user_tz": -330,
          "elapsed": 1,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transformJobsData(project, jobsData):\n",
        "  transformed_data = []\n",
        "\n",
        "  for config in jobsData:\n",
        "    schedule_options = config.get(\"scheduleOptions\", None)\n",
        "    schedule_options_v2 = config.get(\"scheduleOptionsV2\", None)\n",
        "    email_prefermences = config.get(\"emailPreferences\", None)\n",
        "    encryption_configuration = config.get(\"encryptionConfiguration\", None)\n",
        "    params = config.get(\"params\", None)\n",
        "    error = config.get(\"error\", None)\n",
        "    transfer_config = {\n",
        "        \"project\": project,\n",
        "        \"name\": config.get(\"name\", None),\n",
        "        \"display_name\": config.get(\"displayName\", None),\n",
        "        \"data_source_id\": config.get(\"dataSourceId\", None),\n",
        "        \"params\": json.dumps(config.get(\"params\", None)) if config.get(\"params\") else None,\n",
        "        \"schedule\": config.get(\"schedule\", None),\n",
        "        \"schedule_options\": None if not schedule_options else {\n",
        "            \"disable_auto_scheduling\": schedule_options.get(\"disableAutoScheduling\", None),\n",
        "            \"start_time\": schedule_options.get(\"startTime\", None),\n",
        "            \"end_time\": schedule_options.get(\"endTime\", None),\n",
        "        },\n",
        "        \"schedule_options_V2\": None if not schedule_options_v2 else {\n",
        "            \"time_based_schedule\": None if schedule_options_v2.get(\"timeBasedSchedule\") == None else {\n",
        "                \"schedule\": schedule_options_v2.get(\"timeBasedSchedule\").get(\"schedule\", None),\n",
        "                \"start_time\": schedule_options_v2.get(\"timeBasedSchedule\").get(\"startTime\", None),\n",
        "                \"end_time\": schedule_options_v2.get(\"timeBasedSchedule\").get(\"endTime\", None)\n",
        "            },\n",
        "            \"manual_schedule\": schedule_options_v2.get(\"manualSchedule\", None),\n",
        "            \"event_driven_schedule\": None if schedule_options_v2.get(\"eventDrivenSchedule\") == None else {\n",
        "                \"pubsub_subscription\": schedule_options_v2.get(\"eventDrivenSchedule\").get(\"pubsubSubscription\", None)\n",
        "            },\n",
        "        },\n",
        "        \"data_refresh_window_days\": config.get(\"dataRefreshWindowDays\", None),\n",
        "        \"disabled\": config.get(\"disabled\", None),\n",
        "        \"update_time\": config.get(\"updateTime\", None),\n",
        "        \"next_run_time\": config.get(\"nextRunTime\", None),\n",
        "        \"state\": config.get(\"state\", None),\n",
        "        \"user_id\": config.get(\"userId\", None),\n",
        "        \"dataset_region\": config.get(\"datasetRegion\", None),\n",
        "        \"notification_pubsub_topic\": config.get(\"notificationPubsubTopic\", None),\n",
        "        \"email_preferences\": None if not email_prefermences else {\n",
        "            \"enable_failure_email\": email_prefermences.get(\"enableFailureEmail\", None)\n",
        "        },\n",
        "        \"encryption_configuration\": None if not encryption_configuration else {\n",
        "            \"kms_key_name\": encryption_configuration.get(\"kmsKeyName\", None)\n",
        "        },\n",
        "        \"error\": json.dumps(config.get(\"error\", None)) if config.get(\"error\") else None,\n",
        "        \"destination_dataset_id\": config.get(\"destinationDatasetId\", None),\n",
        "        \"owner_info\": config.get(\"ownerInfo\", None)\n",
        "    }\n",
        "    transformed_data.append(transfer_config)\n",
        "  return transformed_data"
      ],
      "metadata": {
        "id": "WbpuApd62GeK",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1741110366250,
          "user_tz": -330,
          "elapsed": 716,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def storeToBQTable(bqClient, projects_to_fetch, project_id, dataset_id):\n",
        "  for project in projects_to_fetch:\n",
        "    print(f'Storing data for project {project}')\n",
        "    data = fetchDataFromApi(project)\n",
        "    if data and \"projectDetails\" in data:\n",
        "      projectData = transformProjectData(data['projectDetails'])\n",
        "      if projectData:\n",
        "        errors = bqClient.insert_rows_json(f'{project_id}.{dataset_id}.project_details', [projectData])\n",
        "        print(f\"Data inserted in project_details for project {project}\")\n",
        "        if errors:\n",
        "          print(f\"Error in storing data into project table: {errors}\")\n",
        "      else:\n",
        "        print(f\"No Data to insert for project details in project {project}\")\n",
        "    if data and \"ancestorDetails\" in data:\n",
        "      ancestorsData = transformAncestorsData(data['ancestorDetails'])\n",
        "      if ancestorsData:\n",
        "        errors = bqClient.insert_rows_json(f'{project_id}.{dataset_id}.ancestor_details', [ancestorsData])\n",
        "        print(f\"Data inserted in ancestor_details for project {project}\")\n",
        "        if errors:\n",
        "          print(f\"Error in storing ancestor details to table: {errors}\")\n",
        "      else:\n",
        "        print(f\"No Data to insert for ancestor details in project: {project}\")\n",
        "    if data and \"scheduledJobDetails\" in data:\n",
        "      transformed_data = transformJobsData(project, data['scheduledJobDetails'])\n",
        "      if transformed_data:\n",
        "        errors = bqClient.insert_rows_json(f'{project_id}.{dataset_id}.scheduled_job_details', transformed_data)\n",
        "        print(f\"Data inserted in job_details for project {project}\")\n",
        "        if errors:\n",
        "          print(f\"Error in storing scheduled job details to table: {errors}\")\n",
        "      else:\n",
        "        print(f\"No Data to insert for scheduled jobs in project {project}\")"
      ],
      "metadata": {
        "id": "NnTYwPIOyrSV",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1741110370260,
          "user_tz": -330,
          "elapsed": 834,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_table(bqClient, query):\n",
        "  query_job = bqClient.query(query)\n",
        "  results = query_job.result()\n",
        "  return results\n",
        "\n",
        "def on_tables_created():\n",
        "  print(\"All tables created successfully\")\n",
        "\n",
        "def create_tables(queries, bqClient):\n",
        "  print(\"##Creating tables##\")\n",
        "  count = 0\n",
        "  # Use ThreadPoolExecutor to run table creation queries concurrently\n",
        "  with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    futures = [executor.submit(create_table, bqClient, query) for query in queries]\n",
        "    # Wait for all tasks to complete\n",
        "    concurrent.futures.wait(futures)\n",
        "\n",
        "    for future in futures:\n",
        "      try:\n",
        "        result = future.result()\n",
        "        count += 1\n",
        "        print(f\"Table created: {result}\")\n",
        "      except Exception as e:\n",
        "        print(f\"Error occured while creating table {e}\")\n",
        "    #on_tables_created()\n",
        "  return count"
      ],
      "metadata": {
        "id": "5TglSX1GNruT",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1741110371781,
          "user_tz": -330,
          "elapsed": 1,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Script Information\n",
        "print(\"To Run this script, user should have the rights on the source project/dataset\"+\n",
        "      \"\\n[fetch the data from project] and target project/dataset [to store data in dataset]\")\n",
        "print(\"This script will try to fetch the data from the following api's\\n\"+\n",
        "      \"1. https://cloudresourcemanager.googleapis.com/v1/projects/{projectId}\\n\"+\n",
        "      \"2. https://cloudresourcemanager.googleapis.com/v1/projects/{projectId}:getAncestry\\n\"+\n",
        "      \"3. https://bigquerydatatransfer.googleapis.com/v1/projects/{projectId}/transferConfigs\\n\")\n",
        "\n",
        "#Inputs from User\n",
        "project_id=input(\"Provide project id to store data shared with unravel.\\n\")\n",
        "dataset_id=input(\"Provide dataset to store data shared with unravel:\\n\")\n",
        "projects_to_fetch = input(\"Provide the list of projects to fetch the data. List of projects must be comma separated\\n\" +\n",
        "                          \"ex: project1, project2, ...\\n\")\n",
        "projects_to_fetch = [project.strip() for project in projects_to_fetch.split(\",\")]\n",
        "print(projects_to_fetch)\n",
        "create_table_queries = [f\"\"\"create table `{project_id}.{dataset_id}.project_details` (project_number string, project_id string, lifecycle_state string,\n",
        "name string, create_time string, labels JSON, parent struct<type string, id string>, tags JSON)\"\"\",\n",
        "                        f\"\"\"create table `{project_id}.{dataset_id}.scheduled_job_details` (\n",
        "project STRING,\n",
        "name STRING,\n",
        "display_name\tSTRING,\n",
        "data_source_id\tSTRING,\n",
        "params JSON,\n",
        "schedule STRING,\n",
        "schedule_options struct<disable_auto_scheduling BOOLEAN, start_time string, end_time string>,\n",
        "schedule_options_V2\tstruct<time_based_schedule struct<schedule string, start_time string, end_time string>, manual_schedule string, event_driven_schedule struct<pubsub_subscription string>>,\n",
        "data_refresh_window_days INTEGER,\n",
        "disabled BOOLEAN,\n",
        "update_time STRING,\n",
        "next_run_time\tSTRING,\n",
        "state\tSTRING,\n",
        "user_id\t\tSTRING,\n",
        "dataset_region\t\tSTRING,\n",
        "notification_pubsub_topic\t\tSTRING,\n",
        "email_preferences struct<enable_failure_email BOOLEAN>,\n",
        "encryption_configuration struct<kms_key_name string>,\n",
        "error struct<code INTEGER, message string, details array<JSON>>,\n",
        "destination_dataset_id STRING,\n",
        "owner_info\tstruct<email string>)\"\"\",\n",
        "                        f\"\"\"create table `{project_id}.{dataset_id}.ancestor_details` (ancestor array<struct<resource_id struct<type string, id string>>>)\"\"\"]\n",
        "client = bigquery.Client()\n",
        "#checks project exsists or not\n",
        "def is_project_exists():\n",
        "  resourceManagerClient = resourcemanager_v3.ProjectsClient()\n",
        "  try:\n",
        "    project = resourceManagerClient.get_project(name=f'projects/{project_id}')\n",
        "    return True\n",
        "  except NotFound:\n",
        "    print(f\"Project {project_id} does not exists. Provide different project\")\n",
        "    return False\n",
        "  except Exception as e:\n",
        "    print(f\"Error getting project status {e}\")\n",
        "    return False\n",
        "\n",
        "#checks dataset exists or not\n",
        "def is_dataset_exists(client):\n",
        "  try:\n",
        "    dataset = client.get_dataset(f\"{project_id}.{dataset_id}\")\n",
        "    return True\n",
        "  except NotFound:\n",
        "    print(f\"Dataset {dataset_id} does not exists.\")\n",
        "    return False\n",
        "\n",
        "#creating dataset to store the table\n",
        "def create_dataset(client):\n",
        "  try:\n",
        "    print(f\"creating dataset {dataset_id}\")\n",
        "    dataset = bigquery.Dataset(f\"{project_id}.{dataset_id}\")\n",
        "    dataset.location = 'US'\n",
        "    dataset = client.create_dataset(dataset)\n",
        "    print(f\"Dataset created {dataset_id}\")\n",
        "    return True\n",
        "  except Exception as e:\n",
        "    print(f\"Error while creating dataset {e}\")\n",
        "    return False\n",
        "\n",
        "# Function to check if the table exists\n",
        "def check_table_exists(bqClient, project_id, dataset_id, table_name):\n",
        "  table_ref = bqClient.dataset(dataset_id).table(table_name)\n",
        "  try:\n",
        "    bqClient.get_table(table_ref)  # Will raise NotFound if table does not exist\n",
        "    return True\n",
        "  except Exception as e:\n",
        "    return False\n",
        "\n",
        "#verifying the tables created or not\n",
        "def verifyTables(client):\n",
        "  tables = ['project_details', 'ancestor_details', 'scheduled_job_details']\n",
        "  time.sleep(5) # sleep for 5 sec by default.\n",
        "  for table in tables:\n",
        "    if not check_table_exists(client, project_id, dataset_id, table):\n",
        "      print(f\"Table {table} not ready yet, retrying...\")\n",
        "      retries = 5\n",
        "      for _ in range(retries):\n",
        "        time.sleep(5)  # Sleep for 5 seconds before retrying\n",
        "        if check_table_exists(client, project_id, dataset_id, table):\n",
        "          print(f\"Table {table} is now available.\")\n",
        "          break\n",
        "        else:\n",
        "          print(f\"Table {table} was not created successfully after retries.\")\n",
        "          return False\n",
        "    else:\n",
        "      print(f\"Table {table} is already available.\")\n",
        "  return True\n",
        "\n",
        "if is_project_exists():\n",
        "  if is_dataset_exists(client) or create_dataset(client):\n",
        "    tableCreatedNumber = create_tables(create_table_queries, client)\n",
        "    if tableCreatedNumber == len(create_table_queries):\n",
        "      if verifyTables(client):\n",
        "        storeToBQTable(client, projects_to_fetch, project_id, dataset_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irDLaFuPLtnx",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1741110414578,
          "user_tz": -330,
          "elapsed": 40262,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "613a2b76-1010-4c88-d7bf-a0ca27b4e7bf"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To Run this script, user should have the rights on the source project/dataset\n",
            "[fetch the data from project] and target project/dataset [to store data in dataset]\n",
            "This script will try to fetch the data from the following api's\n",
            "1. https://cloudresourcemanager.googleapis.com/v1/projects/{projectId}\n",
            "2. https://cloudresourcemanager.googleapis.com/v1/projects/{projectId}:getAncestry\n",
            "3. https://bigquerydatatransfer.googleapis.com/v1/projects/{projectId}/transferConfigs\n",
            "\n",
            "Provide project id to store data shared with unravel.\n",
            "unravel-flat-rate-test\n",
            "Provide dataset to store data shared with unravel:\n",
            "unravel_share_US1\n",
            "Provide the list of projects to fetch the data. List of projects must be comma separated\n",
            "ex: project1, project2, ...\n",
            "unravel-flat-rate-test, unravel-bigquery-dev, unravel-finops\n",
            "['unravel-flat-rate-test', 'unravel-bigquery-dev', 'unravel-finops']\n",
            "Dataset unravel_share_US1 does not exists.\n",
            "creating dataset unravel_share_US1\n",
            "Dataset created unravel_share_US1\n",
            "##Creating tables##\n",
            "Table created: <google.cloud.bigquery.table._EmptyRowIterator object at 0x7c61ae9023e0>\n",
            "Table created: <google.cloud.bigquery.table._EmptyRowIterator object at 0x7c61aea5b4c0>\n",
            "Table created: <google.cloud.bigquery.table._EmptyRowIterator object at 0x7c61aea5a2c0>\n",
            "Table project_details is already available.\n",
            "Table ancestor_details is already available.\n",
            "Table scheduled_job_details is already available.\n",
            "Storing data for project unravel-flat-rate-test\n",
            "fetching data for project unravel-flat-rate-test.\n",
            "Fetch complete for project: unravel-flat-rate-test\n",
            "Data inserted in project_details for project unravel-flat-rate-test\n",
            "Data inserted in ancestor_details for project unravel-flat-rate-test\n",
            "Data inserted in job_details for project unravel-flat-rate-test\n",
            "Storing data for project unravel-bigquery-dev\n",
            "fetching data for project unravel-bigquery-dev.\n",
            "Fetch complete for project: unravel-bigquery-dev\n",
            "Data inserted in project_details for project unravel-bigquery-dev\n",
            "Data inserted in ancestor_details for project unravel-bigquery-dev\n",
            "No Data to insert for scheduled jobs in project unravel-bigquery-dev\n",
            "Storing data for project unravel-finops\n",
            "fetching data for project unravel-finops.\n",
            "Fetch complete for project: unravel-finops\n",
            "Data inserted in project_details for project unravel-finops\n",
            "Data inserted in ancestor_details for project unravel-finops\n",
            "Data inserted in job_details for project unravel-finops\n"
          ]
        }
      ]
    }
  ]
}